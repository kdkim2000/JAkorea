{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdkim2000/JAkorea/blob/main/6__pytorch____neural_style_transfer_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zvbAnyU7gMy"
      },
      "source": [
        "# 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDMi8cOjgpNb"
      },
      "source": [
        "from __future__ import division\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp_sXDNa7jWd"
      },
      "source": [
        "# GPU 사용체크"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjoFiegu7lAO"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugDA-7H48sFV"
      },
      "source": [
        "# 이미지를 불러오는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfxOimtsg3d3"
      },
      "source": [
        "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
        "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    if max_size:\n",
        "        scale = max_size / max(image.size)\n",
        "        size = np.array(image.size) * scale\n",
        "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
        "\n",
        "    if shape:\n",
        "        image = image.resize(shape, Image.LANCZOS)\n",
        "\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image.to(device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medMC8mG8u9O"
      },
      "source": [
        "# 사전학습된 VGGNET을 이용한 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-YcuUYfxqK"
      },
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.select = ['0', '5', '10', '19', '28']\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbRHHG0f84fQ"
      },
      "source": [
        "# 학습과정 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaR0T87u86nw"
      },
      "source": [
        "# 학습에 사용할 이미지 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy1WKZyi0ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce02a15-f02f-46e5-bbfd-db5f780a1eb8"
      },
      "source": [
        "!mkdir png\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg -O png/content.png\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg -O png/style.png"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘png’: File exists\n",
            "--2024-02-10 07:14:26--  https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 2607:f8b0:4023:c0d::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83281 (81K) [image/jpeg]\n",
            "Saving to: ‘png/content.png’\n",
            "\n",
            "png/content.png     100%[===================>]  81.33K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-02-10 07:14:26 (125 MB/s) - ‘png/content.png’ saved [83281/83281]\n",
            "\n",
            "--2024-02-10 07:14:26--  https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 2607:f8b0:4023:c0d::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 195196 (191K) [image/jpeg]\n",
            "Saving to: ‘png/style.png’\n",
            "\n",
            "png/style.png       100%[===================>] 190.62K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-02-10 07:14:26 (133 MB/s) - ‘png/style.png’ saved [195196/195196]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylOFC_lDAQNX"
      },
      "source": [
        "# 설정값 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HOMwx3ph3Cd"
      },
      "source": [
        "# 설정값 지정\n",
        "content = 'png/content.png'\n",
        "style = 'png/style.png'\n",
        "max_size = 400\n",
        "total_step = 500\n",
        "log_step = 100\n",
        "sample_step = 100\n",
        "style_weight = 2000\n",
        "lr = 0.01"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkd5rcD_1t__",
        "outputId": "c64dd574-e993-4399-d675-c322532786f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWTrOQeu_lla"
      },
      "source": [
        "# 트레이닝 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2TjKWw1kDNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff228db5-6be6-4728-f43a-91e445a026fa"
      },
      "source": [
        "# Image preprocessing\n",
        "# VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "# We use the same normalization statistics here.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                          std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "# Load content and style images\n",
        "# Make the style image same size as the content image\n",
        "content = load_image(content, transform, max_size=max_size)\n",
        "style = load_image(style, transform, shape=[content.size(2), content.size(3)])\n",
        "\n",
        "# Initialize a target image with the content image\n",
        "target = content.clone().requires_grad_(True)\n",
        "\n",
        "optimizer = torch.optim.Adam([target], lr=lr, betas=[0.5, 0.999])\n",
        "vgg = VGGNet().to(device).eval()\n",
        "\n",
        "for step in range(total_step):\n",
        "\n",
        "    # Extract multiple(5) conv feature vectors\n",
        "    target_features = vgg(target)\n",
        "    content_features = vgg(content)\n",
        "    style_features = vgg(style)\n",
        "\n",
        "    style_loss = 0\n",
        "    content_loss = 0\n",
        "    for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
        "        # Compute content loss with target and content images\n",
        "        content_loss += torch.mean((f1 - f2)**2)\n",
        "\n",
        "        # Reshape convolutional feature maps\n",
        "        _, c, h, w = f1.size()\n",
        "        f1 = f1.view(c, h * w)\n",
        "        f3 = f3.view(c, h * w)\n",
        "\n",
        "        # Compute gram matrix\n",
        "        f1 = torch.mm(f1, f1.t())\n",
        "        f3 = torch.mm(f3, f3.t())\n",
        "\n",
        "        # Compute style loss with target and style images\n",
        "        style_loss += torch.mean((f1 - f3)**2) / (c * h * w)\n",
        "\n",
        "    # Compute total loss, backprop and optimize\n",
        "    loss = content_loss + style_weight * style_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (step+1) % log_step == 0:\n",
        "        print (f'Step [{step+1}/{total_step}], Content Loss: {content_loss.item():.4f}, Style Loss: {style_loss.item():.4f}')\n",
        "\n",
        "    if (step+1) % sample_step == 0:\n",
        "        # Save the generated image\n",
        "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
        "        img = target.clone().squeeze()\n",
        "        img = denorm(img).clamp_(0, 1)\n",
        "        torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-ae0e545725ab>:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize(size.astype(int), Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [100/500], Content Loss: 52.7428, Style Loss: 245.8949\n",
            "Step [200/500], Content Loss: 58.9113, Style Loss: 89.2688\n",
            "Step [300/500], Content Loss: 61.5896, Style Loss: 48.4805\n",
            "Step [400/500], Content Loss: 63.1516, Style Loss: 33.6707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMP3mlWhELaB"
      },
      "source": [
        "# 생성된 이미지 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBeLxSyiiMjF"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('output-2000.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZT2c7i0GXel"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}