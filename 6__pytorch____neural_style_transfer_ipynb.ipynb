{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdkim2000/JAkorea/blob/main/6__pytorch____neural_style_transfer_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zvbAnyU7gMy"
      },
      "source": [
        "# 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDMi8cOjgpNb"
      },
      "source": [
        "from __future__ import division\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp_sXDNa7jWd"
      },
      "source": [
        "# GPU 사용체크"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjoFiegu7lAO"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugDA-7H48sFV"
      },
      "source": [
        "# 이미지를 불러오는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfxOimtsg3d3"
      },
      "source": [
        "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
        "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    if max_size:\n",
        "        scale = max_size / max(image.size)\n",
        "        size = np.array(image.size) * scale\n",
        "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
        "\n",
        "    if shape:\n",
        "        image = image.resize(shape, Image.LANCZOS)\n",
        "\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medMC8mG8u9O"
      },
      "source": [
        "# 사전학습된 VGGNET을 이용한 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-YcuUYfxqK"
      },
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.select = ['0', '5', '10', '19', '28']\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbRHHG0f84fQ"
      },
      "source": [
        "# 학습과정 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaR0T87u86nw"
      },
      "source": [
        "# 학습에 사용할 이미지 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy1WKZyi0ue"
      },
      "source": [
        "!mkdir png\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg -O png/content.png\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg -O png/style.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylOFC_lDAQNX"
      },
      "source": [
        "# 설정값 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HOMwx3ph3Cd"
      },
      "source": [
        "# 설정값 지정\n",
        "content = 'png/content.png'\n",
        "style = 'png/style.png'\n",
        "max_size = 400\n",
        "total_step = 2000\n",
        "log_step = 10\n",
        "sample_step = 500\n",
        "style_weight = 100\n",
        "lr = 0.003"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWTrOQeu_lla"
      },
      "source": [
        "# 트레이닝 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2TjKWw1kDNh"
      },
      "source": [
        "# Image preprocessing\n",
        "# VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "# We use the same normalization statistics here.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                          std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "# Load content and style images\n",
        "# Make the style image same size as the content image\n",
        "content = load_image(content, transform, max_size=max_size)\n",
        "style = load_image(style, transform, shape=[content.size(2), content.size(3)])\n",
        "\n",
        "# Initialize a target image with the content image\n",
        "target = content.clone().requires_grad_(True)\n",
        "\n",
        "optimizer = torch.optim.Adam([target], lr=lr, betas=[0.5, 0.999])\n",
        "vgg = VGGNet().to(device).eval()\n",
        "\n",
        "for step in range(total_step):\n",
        "\n",
        "    # Extract multiple(5) conv feature vectors\n",
        "    target_features = vgg(target)\n",
        "    content_features = vgg(content)\n",
        "    style_features = vgg(style)\n",
        "\n",
        "    style_loss = 0\n",
        "    content_loss = 0\n",
        "    for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
        "        # Compute content loss with target and content images\n",
        "        content_loss += torch.mean((f1 - f2)**2)\n",
        "\n",
        "        # Reshape convolutional feature maps\n",
        "        _, c, h, w = f1.size()\n",
        "        f1 = f1.view(c, h * w)\n",
        "        f3 = f3.view(c, h * w)\n",
        "\n",
        "        # Compute gram matrix\n",
        "        f1 = torch.mm(f1, f1.t())\n",
        "        f3 = torch.mm(f3, f3.t())\n",
        "\n",
        "        # Compute style loss with target and style images\n",
        "        style_loss += torch.mean((f1 - f3)**2) / (c * h * w)\n",
        "\n",
        "    # Compute total loss, backprop and optimize\n",
        "    loss = content_loss + style_weight * style_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (step+1) % log_step == 0:\n",
        "        print (f'Step [{step+1}/{total_step}], Content Loss: {content_loss.item():.4f}, Style Loss: {style_loss.item():.4f}')\n",
        "\n",
        "    if (step+1) % sample_step == 0:\n",
        "        # Save the generated image\n",
        "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
        "        img = target.clone().squeeze()\n",
        "        img = denorm(img).clamp_(0, 1)\n",
        "        torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMP3mlWhELaB"
      },
      "source": [
        "# 생성된 이미지 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBeLxSyiiMjF"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('output-2000.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZT2c7i0GXel"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}